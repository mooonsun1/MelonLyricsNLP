{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "decades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "\n",
    "##1. 시대별 파일을 가져와서 각각의 변수에 저장 / 결측치 제거 + 말뭉치 생성 / 리스트 형태로 변수에 저장\n",
    "def load_lyrics(decades):\n",
    "    lyrics_1960s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_1970s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_1980s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_1990s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_2000s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_2010s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "    lyrics_2020s = pd.read_csv(\"crawling/1960s.csv\")['가사'].dropna().tolist()\n",
    "\n",
    "    return lyrics_1960s, lyrics_1970s, lyrics_1980s, lyrics_1990s, lyrics_2000s, lyrics_2010s, lyrics_2020s\n",
    "\n",
    "# 불용어 설정\n",
    "stop_words_nltk = set(nltk_stopwords.words('english'))\n",
    "stop_words_nltk.update(['oh', 'uh', 'u', 'na', 'cant', 'dont', 'wan', 'j', 'tu', 'yo', 'la', 'al', 'el', 'hey', 'un', 'ta', 'uaagh', 'aint', 'bam', 'im', 'ah', 'e', 'eh', 'ooh', 'yeah', 'ya', 'woo', 'ba', 'du', 'ti', 'gon', 'imma', 'da']) # 불용어 추가\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    custom_stop_words = f.read().splitlines()\n",
    "stop_words = stop_words_nltk.union(set(custom_stop_words))\n",
    "\n",
    "# 영어 품사 태깅 용어 설정\n",
    "def get_wordnet_tagset(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 정규 표현식 영어 구분, 한국어/영어 토큰화, 품사태깅 및 원형 복원\n",
    "def preprocess_text(text, stop_words):\n",
    "    ko_nouns, ko_adj, en_nouns, en_adj = [], [], [], []\n",
    "\n",
    "    # 1) 정규 표현식으로 한글/영어 구분 및 특수문자 제거\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')\n",
    "    english = re.compile('[^a-zA-Z\\s]')\n",
    "    result_en = english.sub('', text).lower() # 소문자화\n",
    "    result_kr = hangul.sub('', text)\n",
    "\n",
    "    # 2) 영어 토큰화 및 품사 태깅, 원형 복원, 불용어 적용\n",
    "    en_tokens = word_tokenize(result_en) # 토큰화\n",
    "    en_pos_list = pos_tag(en_tokens) # 품사 태깅\n",
    "    \n",
    "    for word, tag in en_pos_list:\n",
    "        # 불용어 설정 및 명사 품사 반환\n",
    "        if get_wordnet_tagset(tag) == 'n' and word.lower() not in stop_words:\n",
    "            en_nouns.append(WordNetLemmatizer().lemmatize(word.lower(), pos='n'))\n",
    "        # 불용어 설정 및 형용사 품사 반환\n",
    "        elif get_wordnet_tagset(tag) == 'a' and word.lower() not in stop_words:\n",
    "            en_adj.append(WordNetLemmatizer().lemmatize(word.lower(), pos='a'))\n",
    "\n",
    "    # 3) 한국어 토큰화 및 품사 태깅, 원형 복원, 불용어 적용\n",
    "    okt = Okt() # okt활용\n",
    "    ko_pos_list = okt.pos(result_kr, stem=True) # 품사 태깅 및 원형 복원\n",
    "    \n",
    "    for word, pos in ko_pos_list:\n",
    "        # 불용어 설정 및 명사 품사 반환\n",
    "        if pos == 'Noun' and word not in stop_words:\n",
    "            ko_nouns.append(word)\n",
    "        # 불용어 설정 및 형용사 품사 반환\n",
    "        elif pos == 'Adjective' and word not in stop_words:\n",
    "            ko_adj.append(word)\n",
    "\n",
    "    return ko_nouns, ko_adj, en_nouns, en_adj\n",
    "\n",
    "def preprocess_data(lyrics_data, stop_words):\n",
    "    # word_counters 딕셔너리 생성\n",
    "    word_counters = {decade: {'ko_nouns': Counter(), 'ko_adj': Counter(), 'en_nouns': Counter(), 'en_adj': Counter()} for decade in decades}\n",
    "    # decades 리스트와 lyrics_data 리스트의 각 항목을 동시에 순회하며 decades가 속하는 가사데이터 처리\n",
    "    for decade, lyrics in zip(decades, lyrics_data):\n",
    "        for text in lyrics:\n",
    "            ko_nouns, ko_adj, en_nouns, en_adj = preprocess_text(text, stop_words)\n",
    "            \n",
    "            word_counters[decade]['ko_nouns'].update(ko_nouns)\n",
    "            word_counters[decade]['ko_adj'].update(ko_adj)\n",
    "            word_counters[decade]['en_nouns'].update(en_nouns)\n",
    "            word_counters[decade]['en_adj'].update(en_adj)\n",
    "\n",
    "    return word_counters\n",
    "\n",
    "# word_counters를 데이터 프레임으로 변환 및 저장\n",
    "def save_to_csv(word_counters):\n",
    "    #데이터 프레임 초기화\n",
    "    df_rows = []\n",
    "\n",
    "    for decade, counters in word_counters.items(): # decades 딕셔너리의 키-값 쌍 순회\n",
    "        for pos, counter in counters.items(): # 빈도수 딕셔너리의 키-값 쌍 순회\n",
    "            language = 'ko' if 'ko' in pos else 'en'\n",
    "            for word, freq in counter.items(): #counter 객체의 키-값 쌍 순회\n",
    "                df_rows.append([decade, word, pos, language, freq])\n",
    "    \n",
    "    df = pd.DataFrame(df_rows, columns=['연대', '단어', '품사', '언어', '빈도수'])\n",
    "    df.to_csv('dataframe/yearly_lyrics.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 데이터 로딩\n",
    "lyrics_data = load_lyrics(decades)\n",
    "\n",
    "# 데이터 전처리\n",
    "word_counters = preprocess_data(lyrics_data, stop_words)\n",
    "\n",
    "# 데이터 저장\n",
    "save_to_csv(word_counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화_WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordCloud' from 'wordcloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# WordCloud\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WordCloud' from 'wordcloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "# WordCloud\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# #깃허브로부터 나눔폰트 다운로드\n",
    "# ! git clone https://github.com/namepen/nanum_font.git\n",
    "\n",
    "# 데이터 로딩\n",
    "df = pd.read_csv('dataframe/yearly_lyrics.csv')\n",
    "\n",
    "# 단어+빈도수 -> vocab으로 만들시 품사, 언어별 wordcloud를 만들기 어려움 \n",
    "def words_counts(df) -> dict:\n",
    "    d = df[['단어', '빈도수']]\n",
    "    d.index = d['단어']\n",
    "    d = d.drop(['단어'], axis=1)\n",
    "    return d.to_dict()['빈도수']\n",
    "\n",
    "def save_wc(filename: str, data: dict, max_font_size=150,  width=512, height=512):\n",
    "        \n",
    "    masking_image = np.array(Image.open(\"images/apple.jpeg\"))\n",
    "    wc = WordCloud(\n",
    "        font_path= \"c:\\\\classes\\\\classes\\\\06_NLP_Preporcessing\\\\nanum_font\\\\NanumGothic-ExtraBold.ttf\", \n",
    "        background_color=\"white\", \n",
    "        max_font_size=max_font_size, \n",
    "        width=width, height=height, \n",
    "        mask=masking_image)\n",
    "    \n",
    "    cloud = wc.generate_from_frequencies(data)\n",
    "    file_path=f\"wordcloud/{filename}.png\"\n",
    "    cloud.to_file(file_path)\n",
    "    return cloud\n",
    "\n",
    "decades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "\n",
    "for decade in decades:\n",
    "    ko = (df['언어'] == 'ko') & (df['연대'] == decade)\n",
    "    en = (df['언어'] == 'en') & (df['연대'] == decade)\n",
    "    ko_noun = (df['품사'] == 'ko_nouns') & (df['연대'] == decade)\n",
    "    ko_adj = (df['품사'] == 'ko_adj') & (df['연대'] == decade)\n",
    "    en_noun = (df['품사'] == 'en_nouns') & (df['연대'] == decade)\n",
    "    en_adj = (df['품사'] == 'en_adj') & (df['연대'] == decade)\n",
    "\n",
    "    # 전체 명사\n",
    "    ko_noun_df = df[ko_noun]\n",
    "    en_noun_df = df[en_noun]\n",
    "\n",
    "    all_noun_df = pd.concat([ko_noun_df, en_noun_df])\n",
    "    save_wc(f'{decade}_all_noun', words_counts(all_noun_df))\n",
    "\n",
    "    # 전체 형용사\n",
    "    ko_adj_df=df[ko_adj]\n",
    "    en_adj_df=df[en_adj]\n",
    "\n",
    "    all_adj_df=pd.concat([ko_adj_df,en_adj_df])\n",
    "    save_wc(f'{decade}s_all_adj', words_counts(all_adj_df))\n",
    "\n",
    "    # 한글\n",
    "    kr_df = df[ko]\n",
    "    save_wc(f'{decade}s_kr', words_counts(kr_df))\n",
    "\n",
    "    # 한글 + 명사\n",
    "    kr_noun = df[ko & ko_noun]\n",
    "    save_wc(f'{decade}s_kr_noun', words_counts(kr_noun))\n",
    "\n",
    "    # 한글 + 형용사\n",
    "    kr_adj = df[ko & ko_adj]\n",
    "    save_wc(f'{decade}s_kr_adj', words_counts(kr_adj))\n",
    "\n",
    "    # 영어\n",
    "    en_df = df[en]\n",
    "    save_wc(f'{decade}s_en', words_counts(en_df))\n",
    "\n",
    "    # 영어 + 명사\n",
    "    en_noun = df[en & en_noun]\n",
    "    save_wc(f'{decade}s_en_noun', words_counts(en_noun))\n",
    "\n",
    "    # 영어 + 형용사\n",
    "    en_adj = df[en & en_adj]\n",
    "    save_wc(f'{decade}s_en_adj', words_counts(en_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화_ 시대별 Top20 단어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "# 데이터 로딩\n",
    "df = pd.read_csv('dataframe/yearly_lyrics.csv')\n",
    "\n",
    "# 각 연대별로 필터링\n",
    "def filter_top_words(df, decade, pos, top=20):\n",
    "    return df[(df['연대'] == decade) & (df['품사'] == pos)].nlargest(top, '빈도수')\n",
    "\n",
    "def plot_top_words(df, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(df['단어'], df['빈도수'], color='skyblue')\n",
    "    plt.xlabel('빈도수')\n",
    "    plt.ylabel('단어')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # 빈도수가 높은 단어부터 표시\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 그래프를 이미지 파일로 저장\n",
    "    save_path = os.path.join('topbar', f'{title}.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "decades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "\n",
    "def save_plots(decades):\n",
    "    if not os.path.exists('topbar'):\n",
    "        os.makedirs('topbar')\n",
    "        \n",
    "    for decade in decades:\n",
    "        df_ko_nouns = filter_top_words(df, decade, 'ko_nouns')\n",
    "        df_ko_adj = filter_top_words(df, decade, 'ko_adj')\n",
    "        df_en_nouns = filter_top_words(df, decade, 'en_nouns')\n",
    "        df_en_adj = filter_top_words(df, decade, 'en_adj')\n",
    "        \n",
    "        plot_top_words(df_ko_nouns, f'{decade} 한국어 명사 Top 20')\n",
    "        plot_top_words(df_ko_adj, f'{decade} 한국어 형용사 Top 20')\n",
    "        plot_top_words(df_en_nouns, f'{decade} 영어 명사 Top 20')\n",
    "        plot_top_words(df_en_adj, f'{decade} 영어 형용사 Top 20')\n",
    "\n",
    "save_plots(decades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화_ 시대별 영어 한국어 비중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "# 데이터 로딩\n",
    "df = pd.read_csv('dataframe/yearly_lyrics.csv')\n",
    "\n",
    "# 누적 막대 그래프를 그리는 함수\n",
    "def plot_stacked_bar(df, title):\n",
    "    # '연대' 컬럼을 기준으로 그룹화\n",
    "    grouped = df.groupby(['연대', '단어']).sum().reset_index()\n",
    "\n",
    "    # 영어와 한국어 빈도를 나타내는 컬럼 생성\n",
    "    grouped['ko'] = grouped.apply(lambda row: row['빈도수'] if row['언어'] == 'ko' else 0, axis=1)\n",
    "    grouped['en'] = grouped.apply(lambda row: row['빈도수'] if row['언어'] == 'en' else 0, axis=1)\n",
    "\n",
    "    # '연대'로 다시 그룹화하여 누적 막대 그래프를 위한 데이터 생성\n",
    "    stacked = grouped.groupby('연대').sum().reset_index()\n",
    "\n",
    "    # 누적 막대 그래프 그리기\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    barWidth = 0.85\n",
    "\n",
    "    r = np.arange(len(stacked['연대']))\n",
    "\n",
    "    plt.bar(r, stacked['ko'], color='#b5ffb9', edgecolor='white', width=barWidth, label='Korean')\n",
    "    plt.bar(r, stacked['en'], bottom=stacked['ko'], color='#f9bc86', edgecolor='white', width=barWidth, label='English')\n",
    "\n",
    "    plt.xlabel('Decades', fontweight='bold')\n",
    "    plt.xticks([r for r in range(len(stacked['연대']))], stacked['연대'])\n",
    "    plt.ylabel('Frequency', fontweight='bold')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'krenbar/{title}.png')\n",
    "    plt.close()\n",
    "\n",
    "# 누적 막대 그래프 그리기\n",
    "plot_stacked_bar(df, 'Language Comparison by Decades')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from konlpy.tag import Okt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 한국어 깨짐 방지\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 한국어 불용어\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    kr_stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "okt = Okt()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 한국어 전처리 함수\n",
    "def preprocess_kor_nouns(text):\n",
    "    ko_text = re.sub(r'[^ㄱ-ㅎ가-힣\\s]+', '', text).strip()\n",
    "    tagged_words = okt.pos(ko_text, stem=True)\n",
    "    filtered_words = [word for word, tag in tagged_words if tag in (['Noun','Adjective']) and word not in kr_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "decades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "\n",
    "tfidf_df = pd.DataFrame()\n",
    "\n",
    "for decade in decades:\n",
    "\n",
    "    # 파일 읽기\n",
    "    df = pd.read_csv(f'crawling/{decade}.csv')\n",
    "    df['가사'] = df['가사'].fillna('')  # 결측값 처리\n",
    "\n",
    "    # 영어와 한국어 분리 -> 전처리\n",
    "    df['lyrics_kr'] = df['가사'].apply(preprocess_kor_nouns)\n",
    "\n",
    "    # 한국어 TF-IDF 변환\n",
    "    tfidf_vectorizer_kr = TfidfVectorizer(max_features=20_000, min_df=5)\n",
    "    tfidf_matrix_kr = tfidf_vectorizer_kr.fit_transform(df['lyrics_kr'])\n",
    "    feature_names_kr = tfidf_vectorizer_kr.get_feature_names_out()\n",
    "\n",
    "    # 각 연대별 TF-IDF 값을 Series로 변환하여 DataFrame에 추가\n",
    "    tfidf_series = pd.Series(tfidf_matrix_kr.toarray().sum(axis=0), index=feature_names_kr, name=decade).sort_values(ascending=False)\n",
    "    tfidf_df = pd.concat([tfidf_df, tfidf_series], axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "tfidf_df.head()\n",
    "\n",
    "# 파일 저장\n",
    "file_path=f\"count_info/tfidf.csv\"\n",
    "tfidf_df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
